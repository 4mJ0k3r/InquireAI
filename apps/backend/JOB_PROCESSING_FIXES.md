# Job Processing Issues - Analysis and Solutions

## Issues Identified

### 1. Out-of-Order Progress Logging (90% appearing after 100%)

**Root Cause:**
- The `updateProgress(90)` call in `processText.service.js` is asynchronous
- The upload worker continues execution and logs "100% - Complete!" before the 90% progress update completes
- This creates a race condition where logs appear out of order

**Solution Applied:**
- Added a 100ms delay after `processTextChunks()` completes to ensure all async progress updates finish
- This ensures proper log ordering: 90% â†’ 100% â†’ "completed successfully"

### 2. Jobs Not Processing Sequentially

**Root Cause:**
- Workers were using default concurrency settings (unlimited concurrent jobs)
- Multiple jobs running simultaneously caused:
  - Race conditions in database operations
  - Resource contention (Redis, MongoDB, Qdrant)
  - Unpredictable job completion order
  - Jobs getting "stuck" due to resource conflicts

**Solution Applied:**
- Added `concurrency: 1` to all document processing workers:
  - Upload worker
  - Google Docs worker  
  - Notion worker
  - Site crawler worker (already had this)
- Added `concurrency: 2` to chat worker (allows some parallelism for better UX)
- Added rate limiting to prevent overwhelming external services

### 3. File Path Resolution Issues (CRITICAL)

**Root Cause:**
- Workers using relative file paths while running from different working directories
- This caused "ENOENT: no such file or directory" errors even when files existed
- The main cause of job failures after implementing concurrency controls

**Solution Applied:**
- Added absolute path resolution in upload worker
- Added file existence checks before processing
- Enhanced error logging for better debugging

## Changes Made

### 1. Upload Worker (`src/workers/upload.worker.js`)
```javascript
// Added delay to fix logging order
await new Promise(resolve => setTimeout(resolve, 100));

// Added concurrency control
}, { 
  connection,
  concurrency: 1, // Process one upload job at a time
  limiter: {
    max: 1, // Max 1 job per second
    duration: 1000
  }
});
```

### 2. Google Docs Worker (`src/workers/gdoc.worker.js`)
```javascript
// Added concurrency control
}, { 
  connection,
  concurrency: 1, // Process one gdoc job at a time
  limiter: {
    max: 1, // Max 1 job per second
    duration: 1000
  }
});
```

### 3. Notion Worker (`src/workers/notion.worker.js`)
```javascript
// Added concurrency control
}, { 
  connection,
  concurrency: 1, // Process one notion job at a time
  limiter: {
    max: 1, // Max 1 job per second
    duration: 1000
  }
});
```

### 4. Chat Worker (`src/workers/chat.worker.js`)
```javascript
// Added moderate concurrency control
}, {
  connection: {
    host: 'localhost',
    port: 6379,
  },
  concurrency: 2, // Allow 2 concurrent chat jobs
  limiter: {
    max: 3, // Max 3 jobs per second
    duration: 1000
  }
});
```

### 5. Enhanced Redis Cleanup (`cleanup-redis.js`)
- Comprehensive queue cleanup
- Removes all job states (waiting, active, completed, failed, delayed)
- Clears repeatable jobs
- Provides detailed cleanup summary

### 6. New Job Monitor (`monitor-jobs.js`)
- Real-time monitoring of all queues
- Shows MongoDB job status
- Displays active, waiting, completed, and failed jobs
- Updates every 3 seconds
- Helps debug job processing issues

## How to Use

### 1. Clean Reset (Recommended)
```bash
# Stop the server first (Ctrl+C)
node cleanup-redis.js
npm run dev
```

### 2. Monitor Jobs in Real-Time
```bash
# In a separate terminal
node monitor-jobs.js
```

### 3. Test Sequential Processing
1. Upload multiple files one after another
2. Observe in the monitor that jobs process sequentially
3. Check logs for proper order: 10% â†’ 20% â†’ 60% â†’ 70% â†’ 80% â†’ 90% â†’ 100% â†’ "completed successfully"

## Expected Behavior After Fixes

### âœ… Correct Log Order
```
ðŸ“Š Job 123 progress: 10% - File read and text extracted
ðŸ“Š Job 123 progress: 20%
ðŸ“Š Job 123 progress: 60%
ðŸ“Š Job 123 progress: 70%
ðŸ“Š Job 123 progress: 80%
ðŸ“Š Job 123 progress: 90%
ðŸ“Š Job 123 progress: 100% - Complete!
âœ… Job 123 completed successfully - 9 chunks embedded and stored
ðŸŽ‰ Worker completed job 1
```

### âœ… Sequential Job Processing
- Job 1 starts and completes fully before Job 2 begins
- No more random job completion patterns
- Consistent processing order
- No stuck jobs in the queue

### âœ… Resource Management
- No more database race conditions
- Proper Redis connection management
- Controlled rate limiting for external APIs
- Better error handling and recovery

## Monitoring and Debugging

### Queue Status Commands
```bash
# Monitor all queues in real-time
node monitor-jobs.js

# Clean all queues and reset state
node cleanup-redis.js

# Check specific job status in MongoDB
node check-jobs.js
```

### Key Metrics to Watch
- **Waiting Jobs**: Should process in order (FIFO)
- **Active Jobs**: Should be limited by concurrency settings
- **Failed Jobs**: Should be minimal with proper error handling
- **Progress Logs**: Should appear in correct sequence

## Performance Considerations

### Trade-offs Made
- **Reduced Throughput**: Sequential processing is slower than parallel
- **Improved Reliability**: Eliminates race conditions and stuck jobs
- **Better Resource Usage**: Prevents overwhelming databases and APIs
- **Predictable Behavior**: Jobs complete in expected order

### When to Adjust Concurrency
- **High Volume**: Increase concurrency for chat worker if needed
- **Resource Limits**: Decrease if experiencing database connection issues
- **API Limits**: Adjust rate limiting based on external service quotas

## Troubleshooting

### If Jobs Still Get Stuck
1. Run `node cleanup-redis.js` to reset queues
2. Check Redis connection: `redis-cli ping`
3. Verify MongoDB connection in logs
4. Monitor with `node monitor-jobs.js`

### If Logs Still Out of Order
1. Increase the delay in upload worker (currently 100ms)
2. Check for other async operations that might interfere
3. Verify all progress updates use await

### If Performance is Too Slow
1. Increase concurrency gradually (start with 2, then 3)
2. Monitor for race conditions with `node monitor-jobs.js`
3. Adjust rate limiting if external APIs allow higher rates

## Next Steps

1. **Test the fixes** by uploading multiple documents
2. **Monitor job processing** with the new monitoring script
3. **Adjust concurrency** if needed based on your specific requirements
4. **Set up alerts** for failed jobs in production
5. **Consider implementing** job retry mechanisms for better resilience